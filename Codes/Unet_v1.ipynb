{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train U-Net on the HR Global Landslide Detection Dataset\n",
    "\n",
    "*Authors: Sansar Raj Meena, Lorenzo Nava, Kushanav Bhuyan, and Lucas Soares*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from losses import dice_loss\n",
    "# Library with segmentation metrics\n",
    "import segmentation_models as sm\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "if tf.test.gpu_device_name(): \n",
    "\n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "else:\n",
    "\n",
    "   print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load(f'data/arrays/trainX.npy')\n",
    "Y_train = np.load(f'data/arrays/trainY.npy')\n",
    "X_val = np.load(f'data/arrays/valX.npy')\n",
    "Y_val = np.load(f'data/arrays/valY.npy')\n",
    "X_test = np.load(f'data/arrays/testX.npy')\n",
    "Y_test = np.load(f'data/arrays/testY.npy')\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualise some data\n",
    "for i in range(5):\n",
    "    f, axarr = plt.subplots(1,2,figsize=(8,8))\n",
    "    axarr[0].imshow(X_train[i][:,:,:3])\n",
    "    axarr[1].imshow(np.squeeze(Y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the evaluation metrics - Precision, Recall, FScore, IoU\n",
    "metrics = [sm.metrics.Precision(threshold=0.5),sm.metrics.Recall(threshold=0.5),sm.metrics.FScore(threshold=0.5,beta=1),sm.metrics.IOUScore(threshold=0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training - Results are saved in a .csv file\n",
    "\n",
    "# Size of the tiles\n",
    "size = X_train.shape[2]\n",
    "# Image bands\n",
    "img_bands = X_train.shape[3]\n",
    "# Sampling method\n",
    "sampling = \"no_overlap\"\n",
    "# Loss function\n",
    "loss=dice_loss\n",
    "\n",
    "# Number of filters \n",
    "filters = [4, 8, 16, 32]\n",
    "# Learning rates\n",
    "lr = [10e-3, 5e-4, 10e-4, 5e-5, 10e-5]\n",
    "# Batch sizes \n",
    "batch_size = [4, 8, 16, 32]\n",
    "# Epochs\n",
    "epochs = 5\n",
    "\n",
    "# Dictionary that will save the results\n",
    "dic = {}\n",
    "\n",
    "# Hyperparameters\n",
    "dic[\"model\"] = []\n",
    "dic[\"batch_size\"] = []\n",
    "dic[\"learning_rate\"] = []\n",
    "dic[\"filters\"] = []\n",
    "\n",
    "# test area 1\n",
    "dic[\"precision_area\"] = []\n",
    "dic[\"recall_area\"] = []\n",
    "dic[\"f1_score_area\"] = []\n",
    "dic[\"iou_score_area\"] = []\n",
    "\n",
    "# loop over all the filters in the filter list\n",
    "for fiilter in filters:\n",
    "    # loop over the learning rates\n",
    "    for learning_rate in lr:\n",
    "        # loop over all batch sizes in batch_size list\n",
    "        for batch in batch_size:\n",
    "            print('_______________________________________________________________________________')\n",
    "            print('Filters: ', fiilter)\n",
    "            print('Learning rate: ', learning_rate)\n",
    "            print('Batch size: ', batch)\n",
    "            \n",
    "            def unet(lr,filtersFirstLayer, pretrained_weights = None,input_size = (size,size,img_bands)):\n",
    "                inputs = Input(input_size)\n",
    "                conv1 = Conv2D(filtersFirstLayer, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(inputs)\n",
    "                conv1 = Conv2D(filtersFirstLayer, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv1)\n",
    "                pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "                conv2 = Conv2D(filtersFirstLayer*2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(pool1)\n",
    "                conv2 = Conv2D(filtersFirstLayer*2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv2)\n",
    "                pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "                conv3 = Conv2D(filtersFirstLayer*4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(pool2)\n",
    "                conv3 = Conv2D(filtersFirstLayer*4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv3)\n",
    "                pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "                conv4 = Conv2D(filtersFirstLayer*8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(pool3)\n",
    "                conv4 = Conv2D(filtersFirstLayer*8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv4)\n",
    "                pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "                conv5 = Conv2D(filtersFirstLayer*16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(pool4)\n",
    "                conv5 = Conv2D(filtersFirstLayer*16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv5)\n",
    "\n",
    "                up6 = Conv2D(filtersFirstLayer*8, 2, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(UpSampling2D(size = (2,2))(conv5))\n",
    "                merge6 = concatenate([conv4,up6], axis = 3)\n",
    "                conv6 = Conv2D(filtersFirstLayer*8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(merge6)\n",
    "                conv6 = Conv2D(filtersFirstLayer*8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv6)\n",
    "\n",
    "                up7 = Conv2D(filtersFirstLayer*4, 2, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "                merge7 = concatenate([conv3,up7], axis = 3)\n",
    "                conv7 = Conv2D(filtersFirstLayer*4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(merge7)\n",
    "                conv7 = Conv2D(filtersFirstLayer*4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv7)\n",
    "\n",
    "                up8 = Conv2D(filtersFirstLayer*2, 2, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "                merge8 = concatenate([conv2,up8], axis = 3)\n",
    "                conv8 = Conv2D(filtersFirstLayer*2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(merge8)\n",
    "                conv8 = Conv2D(filtersFirstLayer*2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv8)\n",
    "\n",
    "                up9 = Conv2D(filtersFirstLayer, 2, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "                merge9 = concatenate([conv1,up9], axis = 3)\n",
    "                conv9 = Conv2D(filtersFirstLayer, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(merge9)\n",
    "                conv9 = Conv2D(filtersFirstLayer, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv9)\n",
    "                conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv9)\n",
    "                conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "                model = Model(inputs, conv10)\n",
    "\n",
    "                model.compile(optimizer = Adam(learning_rate = lr), loss = loss, metrics = metrics)\n",
    "\n",
    "                #model.summary()\n",
    "\n",
    "                if(pretrained_weights):\n",
    "                    model.load_weights(pretrained_weights)\n",
    "\n",
    "                return model\n",
    "\n",
    "            # Load the model\n",
    "            model = unet(filtersFirstLayer= fiilter, lr = learning_rate, input_size = (size,size,img_bands))\n",
    "            # Stop the training if the validation loss does not decrease after 30 epochs\n",
    "            early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', # what is the metric to measure\n",
    "                              patience = 30, # how many epochs to continue running the model after seeing an increase in val_loss\n",
    "                              restore_best_weights = True) # update the mod\n",
    "            # Save the models only when validation loss decrease\n",
    "            model_checkpoint = tf.keras.callbacks.ModelCheckpoint(f'data/model/unet/weights/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}.hdf5',\n",
    "                                                                  monitor='val_loss', mode='min',verbose=0, save_best_only=True,save_weights_only = True)\n",
    "            # fit the model 20% of the dataset was used as validation\n",
    "            history = model.fit(X_train,Y_train,batch_size = batch,epochs=epochs,validation_split=0.2,\n",
    "                                callbacks = [model_checkpoint, early_stop], verbose=1)\n",
    "\n",
    "            # summarize history for iou score\n",
    "            plt.plot(history.history['f1-score'])\n",
    "            plt.plot(history.history['val_f1-score'])\n",
    "            plt.title('model f1-score')\n",
    "            plt.ylabel('f1-score')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            # save plots \n",
    "            plt.savefig(f\"data/model/unet/plots/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}_iou_score.png\")\n",
    "            plt.show()\n",
    "            # summarize history for loss\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            plt.savefig(f\"data/model/unet/plots/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}_val_loss.png\")\n",
    "            plt.show()\n",
    "            \n",
    "            # load unet to evaluate the test data\n",
    "            attn_unet = unet(filtersFirstLayer= fiilter, lr = learning_rate,input_size=(size,size,img_bands))\n",
    "            # load the last saved weight from the training\n",
    "            attn_unet.load_weights(f\"data/model/unet/weights/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}.hdf5\")\n",
    "            \n",
    "           # Evaluate the model\n",
    "            res_1= attn_unet.evaluate(X_test,Y_test)\n",
    "\n",
    "\n",
    "            # save results on the dictionary\n",
    "            dic[\"model\"].append(\"Unet\")\n",
    "            dic[\"batch_size\"].append(batch)\n",
    "            dic[\"learning_rate\"].append(learning_rate)\n",
    "            dic[\"filters\"].append(fiilter)\n",
    "            dic[\"precision_area\"].append(res_1[1])\n",
    "            dic[\"recall_area\"].append(res_1[2])\n",
    "            dic[\"f1_score_area\"].append(res_1[3])\n",
    "            dic[\"iou_score_area\"].append(res_1[4])\n",
    "            \n",
    "            # Convert results to a dataframe\n",
    "            results = pd.DataFrame(dic)\n",
    "            # Export as csv\n",
    "            results.to_csv(f'data/model/unet/results/results_Unet.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the best model based on the best performances on the test set (check CSV file)\n",
    "# Loading the model weights\n",
    "unet_best = unet(filtersFirstLayer= 4,lr = 0.0005,input_size=(size,size,img_bands))\n",
    "unet_best.load_weights(\"data/model/unet/weights/unet_no_overlap_size_128_filters_4_batch_size_16_lr_0.0005.hdf5\")\n",
    "\n",
    "# Plot predictions on test set\n",
    "for i in range(X_test.shape[0]):\n",
    "    preds_train_1 = unet_best.predict(np.expand_dims(X_test[i],axis = 0), verbose=0)\n",
    "    # It's possible to change the 0.5 threshold to improve the results;\n",
    "    preds_train_t1 = (preds_train_1 > 0.5).astype(np.uint8)\n",
    "    f, axarr = plt.subplots(1,3,figsize=(10,10))\n",
    "    axarr[0].imshow(X_test[i][:,:,:3])\n",
    "    axarr[1].imshow(np.squeeze(preds_train_t1))\n",
    "    axarr[2].imshow(np.squeeze(Y_test[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c5906200580bdc572e1d260bc7d196a2bb4943342f05a4580bbfcb76fbea5878"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
